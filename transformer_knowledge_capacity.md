# Transformer reasoning and knowledge capacity

Zeyuan Allen-Zhu and Yuanzhi Li studied the ``knowledge capacity'' of transformers in their paper [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://arxiv.org/abs/2404.05405). They trained a tranformer to memorize individual's attributes in fictional dataset of biographical profiles. The profiles contain:
 - A random name, generated by uniformly sampling a first, middle and last name from a long list of names (it's not quite uniform - duplicate full names are rejected, but the number of possible names is >> the number of profiles)
 - A collection of uniformly random attributes

They generate collections of profiles of different size, and train transformers of different sizes to memorize the names and attributes. They find that transformers predictably memorize 2 bits of information per parameter, a figure that is basically unchanged between 16 and 32bit parameters (but is much lower for 8bit parameters).

Under their scheme, there is an easily computable lower bound on the amount of information required to describe a profile. Modulo some fixed overhead for encoding "what a profile looks like", we require $$log(n_1) + log(n_2) + log(n_3) + Klog(m)$$ bits to describe a profile, where $n_1, n_2, n_3$ are the number of possible first, middle and last names, $m$ is the number of possible values for each attribute and $K$ the number of different attributes. If there are $N$ times this number of profiles, then we can say that, at minimum, $$N(log(n_1) + log(n_2) + log(n_3) + Klog(m))$$ bits are required to memorize the dataset.

This is a lower bound - it's always possible to be less efficient. Given that the best schema for encoding this data generating process is very straightforward (basically: learn the attributes, then learn which attributes go with which names), it may be the case that this is where transformers are about as efficient as they are able to be.

We're interested in studying the knowledge capacity of transformers on data generating processes that are more complex than this. For example, consider the "two-hop QA" DGP. Here, the dataset consists of a collection of questions like "Who is the employer of the brother of Paul Raimondo Eckletree?", and the answer is based on a database of randomly generated names, attributes and relationships between profiles. Optimally, we can store only each profile once with a similar encoding scheme as the original biographical dataset. However, we believe that transformers will in fact have to store profiles twice: they have to "look up" the answer to the first "hop" of the question, and then use that answer to look up the answer to the second hop. Because they cannot reuse layers, the lookup function will have to be stored in two different layers.

We're interested in compression efficiency of transformers for a few reasons:

 - We might be able to establish bounds on how well transformers can compress some data generating processes of interest; for example, it might be unfeasible for transformers to learn "many-hop" reasoning (for sufficiently large hops & data complexity) even if the transformer is very large.
 - This might allow us to study inductive biases of transformers. We could hypothesize that if transformers don't efficiently compress some DGP, they may also be slow to learn it.

## Unresolved questions

 - Should we compare equal training steps or equal number of presentations? (Allen-Zhu & Li use equal presentations.)
 

## Two hop knowledge capacity

